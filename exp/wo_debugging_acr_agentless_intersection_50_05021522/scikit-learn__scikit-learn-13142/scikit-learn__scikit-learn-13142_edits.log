### sklearn/mixture/base.py
<<<<<<< SEARCH
            self._m_step(X, log_resp)
            lower_bound = self._compute_lower_bound(
                log_resp, log_prob_norm)
=======
            self._m_step(X, log_resp)
            lower_bound = self._compute_lower_bound(
                log_resp, log_prob_norm)

            # Use a final e-step for consistency between fit_predict and predict
            _, log_resp = self._e_step(X)
>>>>>>> REPLACE

### sklearn/mixture/base.py
<<<<<<< SEARCH
        return log_resp.argmax(axis=1)
=======
        self._set_parameters(best_params)
        self.n_iter_ = best_n_iter
        self.lower_bound_ = max_lower_bound

        # Ensure predict returns consistent labels
        return log_resp.argmax(axis=1)
>>>>>>> REPLACE
